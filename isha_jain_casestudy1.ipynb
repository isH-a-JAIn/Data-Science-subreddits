{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07588ab",
   "metadata": {},
   "source": [
    "## Case Study 1\n",
    "#### Note: This should be completed individually.\n",
    "\n",
    "Assume you are new to the data science field, and you want to find out what real practitioners and soon-to-be data scientists are concerned about. One place where you may find such information is X (formerly known as Twitter). However, X users often use their real identities and may have reservations about sharing all their opinions publicly. Another place where such information maybe found is the datascience subreddit on Reddit.com (https://www.reddit.com/r/datascience/). Users are assumed to be anonymous and they are more likely to share their opinions without reservations. To find out common concerns among the datascience subreddit users, it might be a good idea to collect the top 100 posts in the subreddit in the year 2023. You might also collect the top 3 comments of each of those posts. In this case study, we will do exactly that. Specific details can be found in the next few cells. \n",
    "\n",
    "This data can be used for many different projects. However, we are only going to focus on the \"data gathering\" part. We will also do some cleaning.\n",
    "\n",
    "**Note**: This case study contributes 10% to your overall grade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4b624",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "###  15 points\n",
    "\n",
    "\n",
    "**Description:** \n",
    "\n",
    "Learn about the **PRAW** package for Python and learn how you can use it to load reddit posts, comments etc. on a Jupyter Notebook. Do a Google search. You might find tutorials. It is okay to use them. You may need to use secret keys for this part. For that you will need to open a Reddit account. You can use a throwaway account for this purpose. Write your code in the cell below. Any code you write to retrieve data from Reddit can go there.\n",
    "\n",
    "Here is a link to the PRAW documentation: https://praw.readthedocs.io/en/stable/#getting-started\n",
    "\n",
    "**Grading criteria:** \n",
    "\n",
    "The code for this step must be correct. Otherwise, the next steps cannot be completed. In that case, the next steps will not be graded. If you receive a praw object from the data science subreddit, you will get full 15 points.' Other methods may be considered, but not encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf6c1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.models.listing.generator.ListingGenerator object at 0x0000024D759BCB20>\n"
     ]
    }
   ],
   "source": [
    "# your code for step 1 goes here\n",
    "\n",
    "#pip install praw\n",
    "#pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip\n",
    "\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='mVQ9Pk6cuqPgOgZwlKd6Hw',\n",
    "                     client_secret='hMLeBBi-M754Ye5N2RjM3oE7ZimATg',\n",
    "                     password='LSFuzzy4510!', \n",
    "                     user_agent='python_praw',\n",
    "                     username='Bruno4510')\n",
    "\n",
    "subreddit=reddit.subreddit('datascience')\n",
    "top=subreddit.top(limit=5)\n",
    "print(top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea70d5",
   "metadata": {},
   "source": [
    "## Step 2: \n",
    "### 10 + 20 + 10 + 15 + 5 + 5 = 65 points\n",
    "\n",
    "**Description**:\n",
    "Once you have the mechanism in place to retrieve data from Reddit, you next step is to determine which parts of the data is necessary. For this case study, collect only the top posts from the year 2023. Also consider if the score of each post was above 50 or not. If the score was below 50, it might not have been an important post. Do not consider those posts. \n",
    "\n",
    "You may also observe that sometimes posts with memes or jokes get a lot of 'upvotes,' and because of that they may  have high scores, but they may not be useful for this case study. To address this problem, you will simply get rid of any post that has fewer than 5 words in the title. \n",
    "\n",
    "You will also notice that praw returns time as an integer. It is inconvenient for us to read time like that. You may want to convert the integer time to human readable time. You do not need to mention hours, minutes, or seconds. Just year, month and date is enough.\n",
    "\n",
    "**Grading Criteria:**\n",
    "* posts are only from the year 2023: 10 points\n",
    "* the integer time format converted into year-month-day: 20 points\n",
    "* only posts with scores more than 50 were considered: 10 points\n",
    "* only post titles with more than 5 words were kept: 15 points\n",
    "* minimum 100 posts were collected: 5 points\n",
    "* three comments collected for each post: 5 points\n",
    "\n",
    "Note: All six grading criteria can be satified by writing one line or many lines of code. It does not matter. As long as your code satisfies the six criteria (in one line or many lines), you will get full points. Otherwise, you will get partial credits.\n",
    "\n",
    "Also note: In case the API does not allow you to collect 100 posts, you can collect 80 or 90 etc. In that case, please copy and paste the error message in a new cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fef2a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post title</th>\n",
       "      <th>Post Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>ID</th>\n",
       "      <th>Post score</th>\n",
       "      <th>Upvote Ratio</th>\n",
       "      <th>Total Comments</th>\n",
       "      <th>Post URL</th>\n",
       "      <th>Original Content</th>\n",
       "      <th>Top comment 1</th>\n",
       "      <th>Top comment 2</th>\n",
       "      <th>Top comment 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a hiring manager - this, this right here</td>\n",
       "      <td></td>\n",
       "      <td>2023-01-27</td>\n",
       "      <td>10mmm38</td>\n",
       "      <td>2624</td>\n",
       "      <td>0.97</td>\n",
       "      <td>135</td>\n",
       "      <td>https://i.redd.it/fk95v2ghilea1.png</td>\n",
       "      <td>False</td>\n",
       "      <td>He also has a PhD in mathematics so I'm sure t...</td>\n",
       "      <td>I‚Äôm assuming he had a strong mathematical/stat...</td>\n",
       "      <td>How seriously are personal projects taken? I'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pretty Accurate Chart to Clear Up Job Title Am...</td>\n",
       "      <td></td>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>12zwc24</td>\n",
       "      <td>1971</td>\n",
       "      <td>0.94</td>\n",
       "      <td>199</td>\n",
       "      <td>https://i.redd.it/1t5iwk9ymbwa1.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>&gt; _Yeah, well, that's just, like, your opinion...</td>\n",
       "      <td>\\nChart : Ambiguous\\nJob Title Ambiguities : e...</td>\n",
       "      <td>I've done all of those things, in varying amou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300,000+ Tech jobs have been vanished in the l...</td>\n",
       "      <td></td>\n",
       "      <td>2023-01-20</td>\n",
       "      <td>10h4zfl</td>\n",
       "      <td>1379</td>\n",
       "      <td>0.92</td>\n",
       "      <td>182</td>\n",
       "      <td>https://i.redd.it/x9hvdw9rdada1.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Based on your chart it's 200k+ in the past six...</td>\n",
       "      <td>Someone else posted something similar but inst...</td>\n",
       "      <td>If my data viz was this bad I would deserve it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which programming language is required for a...</td>\n",
       "      <td></td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>11d4uys</td>\n",
       "      <td>1235</td>\n",
       "      <td>0.84</td>\n",
       "      <td>238</td>\n",
       "      <td>https://i.redd.it/bdmbfzxtvpka1.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>It's a shame you didn't copy the FAIRLY useful...</td>\n",
       "      <td>It's never matlab, why did my PhD supervisor m...</td>\n",
       "      <td>Php ü§£ü§£ü§£ has the author learned php and trying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Everyone here seems focused on advanced modell...</td>\n",
       "      <td>I‚Äôve been browsing this sub for over 5 years. ...</td>\n",
       "      <td>2023-03-18</td>\n",
       "      <td>11uzhqa</td>\n",
       "      <td>1195</td>\n",
       "      <td>0.95</td>\n",
       "      <td>190</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>I've lost track of how many DS contracts are: ...</td>\n",
       "      <td>Data analytics.. that‚Äôs data analytics</td>\n",
       "      <td>True, but I would argue in general, if you're ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I investigated the Underground Economy of Glas...</td>\n",
       "      <td>Online company reviews are high stakes.\\n\\nTop...</td>\n",
       "      <td>2023-05-15</td>\n",
       "      <td>13ilm03</td>\n",
       "      <td>1152</td>\n",
       "      <td>0.99</td>\n",
       "      <td>63</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>Very thorough analysis OP. I wonder if there‚Äôs...</td>\n",
       "      <td>Incredibly thorough analysis. And that's why I...</td>\n",
       "      <td>This is a great investigation. Wow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When Pandas.read_csv \"helpfully\" guesses the d...</td>\n",
       "      <td></td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>11ddeft</td>\n",
       "      <td>1121</td>\n",
       "      <td>0.97</td>\n",
       "      <td>23</td>\n",
       "      <td>https://i.redd.it/tppr6p77tqka1.png</td>\n",
       "      <td>False</td>\n",
       "      <td>If you open it in Excel first, he'd be Agent J...</td>\n",
       "      <td>The further I get into ML and data engineering...</td>\n",
       "      <td>That dummy 1 is obviously a float not an int.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The true reason I chose to be a DS..</td>\n",
       "      <td></td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>10de0j4</td>\n",
       "      <td>1033</td>\n",
       "      <td>0.91</td>\n",
       "      <td>27</td>\n",
       "      <td>https://i.redd.it/2te69la63gca1.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>I'm not a DS, I'm a fleshy Reinforcement Algor...</td>\n",
       "      <td>What the hell man I just got here</td>\n",
       "      <td>I did not need to be called out like this today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Very simple guys. This is the way to go.</td>\n",
       "      <td></td>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>1200b4s</td>\n",
       "      <td>1033</td>\n",
       "      <td>0.85</td>\n",
       "      <td>253</td>\n",
       "      <td>https://i.redd.it/aspuqjwxhkpa1.png</td>\n",
       "      <td>False</td>\n",
       "      <td>I love when people who have a relevant degree ...</td>\n",
       "      <td>I mean that would be great, if it was any less...</td>\n",
       "      <td>It‚Äôs very r/restofthefuckingowl kind of advice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Changing my feminine first name to a masculine...</td>\n",
       "      <td>Just a heads up to any other women that this c...</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>1032pgs</td>\n",
       "      <td>974</td>\n",
       "      <td>0.87</td>\n",
       "      <td>226</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>I have a female cheer leader name. I once had ...</td>\n",
       "      <td>I had the same experience with legally changin...</td>\n",
       "      <td>What‚Äôs crazy is I have the opposite problem. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I hire for super senior data scientists (30+ y...</td>\n",
       "      <td>First, I always ask facts about the Sun. How m...</td>\n",
       "      <td>2023-03-17</td>\n",
       "      <td>11u1xb7</td>\n",
       "      <td>881</td>\n",
       "      <td>0.86</td>\n",
       "      <td>228</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>I'm surprised you don't ask if they can fix yo...</td>\n",
       "      <td>Weirdly at the end of my physics degree we had...</td>\n",
       "      <td>Is this a reference to some odd post in this s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I posted for a Data Analyst, this is what you ...</td>\n",
       "      <td>Our org needs a new data analyst, so I wrote u...</td>\n",
       "      <td>2023-05-17</td>\n",
       "      <td>13k8665</td>\n",
       "      <td>860</td>\n",
       "      <td>0.92</td>\n",
       "      <td>286</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>So I guess the obvious takeaway is that there'...</td>\n",
       "      <td>How will I get my Data science job with all th...</td>\n",
       "      <td>&gt;Two were overqualified. Their experience was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Against all stigma, I love being a SQL monkey!</td>\n",
       "      <td>A year ago I landed a job at an F50 company t...</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>11nwxd6</td>\n",
       "      <td>862</td>\n",
       "      <td>0.97</td>\n",
       "      <td>159</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>Some people here for some reason love to bitch...</td>\n",
       "      <td>It took me 8 years to get to six figures. What...</td>\n",
       "      <td>Most of the people looking down on SQL Monkeys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ever disassociate during job interviews becaus...</td>\n",
       "      <td>I was sitting there yesterday on a video call ...</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>12dhmus</td>\n",
       "      <td>859</td>\n",
       "      <td>0.90</td>\n",
       "      <td>246</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>If morality is something your struggling with,...</td>\n",
       "      <td>&gt;Isn't that what your real mission is? Even in...</td>\n",
       "      <td>I felt exactly what you described in many inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Calling all NLP gurus, Meta is paying top doll...</td>\n",
       "      <td></td>\n",
       "      <td>2023-02-11</td>\n",
       "      <td>10zvb04</td>\n",
       "      <td>851</td>\n",
       "      <td>0.96</td>\n",
       "      <td>73</td>\n",
       "      <td>https://i.redd.it/ywof5cgsgnha1.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>This ad was created by the previous NLP engineer.</td>\n",
       "      <td>BAHAHAHAHA .. after all the recent layoffs the...</td>\n",
       "      <td>US $?\\nOr meta $?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Microsoft is bringing Python to Excel</td>\n",
       "      <td>[https://www.theverge.com/2023/8/22/23841167/m...</td>\n",
       "      <td>2023-08-22</td>\n",
       "      <td>15y7j15</td>\n",
       "      <td>767</td>\n",
       "      <td>0.98</td>\n",
       "      <td>116</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>Feel like this has been hyped forever. Excited...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>A million IT Security engineers suddenly and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Anyone else been mildly horrified once they di...</td>\n",
       "      <td>I'm a few months into my first job as a data a...</td>\n",
       "      <td>2023-05-21</td>\n",
       "      <td>13o7m3d</td>\n",
       "      <td>731</td>\n",
       "      <td>0.98</td>\n",
       "      <td>234</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>Well my company just gave me completely unanon...</td>\n",
       "      <td>You would be absolutely horrified if you looke...</td>\n",
       "      <td>That's more than mildly horrifying.  If you lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A small rant - The quality of data analysts / ...</td>\n",
       "      <td>I work for a mid size company as a manager and...</td>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>14k6qt5</td>\n",
       "      <td>710</td>\n",
       "      <td>0.87</td>\n",
       "      <td>597</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>Hilarious ü§£ although i find scatter plots quit...</td>\n",
       "      <td>It's funny because I can answer most of these ...</td>\n",
       "      <td>We use a P-value of 0.05 because R.A Fisher to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>When stakeholders change their mind on the met...</td>\n",
       "      <td></td>\n",
       "      <td>2023-04-23</td>\n",
       "      <td>12wi8tg</td>\n",
       "      <td>688</td>\n",
       "      <td>0.98</td>\n",
       "      <td>38</td>\n",
       "      <td>https://i.redd.it/i1oqiifhklva1.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>98% too high\\n\\n12% too low \\n\\n65% is clearly...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>ChatGPT gone wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pandas 2.0 is going live, and Apache Arrow wil...</td>\n",
       "      <td>With Pandas 2.0, no existing code should break...</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>12dbhsg</td>\n",
       "      <td>663</td>\n",
       "      <td>0.98</td>\n",
       "      <td>72</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>False</td>\n",
       "      <td>Thanks for this helpful post!</td>\n",
       "      <td>The performance shittiness of pandas is a larg...</td>\n",
       "      <td>Polars ‚úåÔ∏è</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Post title  \\\n",
       "0         As a hiring manager - this, this right here   \n",
       "1   Pretty Accurate Chart to Clear Up Job Title Am...   \n",
       "2   300,000+ Tech jobs have been vanished in the l...   \n",
       "3     Which programming language is required for a...   \n",
       "4   Everyone here seems focused on advanced modell...   \n",
       "5   I investigated the Underground Economy of Glas...   \n",
       "6   When Pandas.read_csv \"helpfully\" guesses the d...   \n",
       "7                The true reason I chose to be a DS..   \n",
       "8            Very simple guys. This is the way to go.   \n",
       "9   Changing my feminine first name to a masculine...   \n",
       "10  I hire for super senior data scientists (30+ y...   \n",
       "11  I posted for a Data Analyst, this is what you ...   \n",
       "12     Against all stigma, I love being a SQL monkey!   \n",
       "13  Ever disassociate during job interviews becaus...   \n",
       "14  Calling all NLP gurus, Meta is paying top doll...   \n",
       "15              Microsoft is bringing Python to Excel   \n",
       "16  Anyone else been mildly horrified once they di...   \n",
       "17  A small rant - The quality of data analysts / ...   \n",
       "18  When stakeholders change their mind on the met...   \n",
       "19  Pandas 2.0 is going live, and Apache Arrow wil...   \n",
       "\n",
       "                                            Post Text        Date       ID  \\\n",
       "0                                                      2023-01-27  10mmm38   \n",
       "1                                                      2023-04-26  12zwc24   \n",
       "2                                                      2023-01-20  10h4zfl   \n",
       "3                                                      2023-02-27  11d4uys   \n",
       "4   I‚Äôve been browsing this sub for over 5 years. ...  2023-03-18  11uzhqa   \n",
       "5   Online company reviews are high stakes.\\n\\nTop...  2023-05-15  13ilm03   \n",
       "6                                                      2023-02-27  11ddeft   \n",
       "7                                                      2023-01-16  10de0j4   \n",
       "8                                                      2023-03-23  1200b4s   \n",
       "9   Just a heads up to any other women that this c...  2023-01-04  1032pgs   \n",
       "10  First, I always ask facts about the Sun. How m...  2023-03-17  11u1xb7   \n",
       "11  Our org needs a new data analyst, so I wrote u...  2023-05-17  13k8665   \n",
       "12   A year ago I landed a job at an F50 company t...  2023-03-10  11nwxd6   \n",
       "13  I was sitting there yesterday on a video call ...  2023-04-06  12dhmus   \n",
       "14                                                     2023-02-11  10zvb04   \n",
       "15  [https://www.theverge.com/2023/8/22/23841167/m...  2023-08-22  15y7j15   \n",
       "16  I'm a few months into my first job as a data a...  2023-05-21  13o7m3d   \n",
       "17  I work for a mid size company as a manager and...  2023-06-27  14k6qt5   \n",
       "18                                                     2023-04-23  12wi8tg   \n",
       "19  With Pandas 2.0, no existing code should break...  2023-04-06  12dbhsg   \n",
       "\n",
       "    Post score  Upvote Ratio  Total Comments  \\\n",
       "0         2624          0.97             135   \n",
       "1         1971          0.94             199   \n",
       "2         1379          0.92             182   \n",
       "3         1235          0.84             238   \n",
       "4         1195          0.95             190   \n",
       "5         1152          0.99              63   \n",
       "6         1121          0.97              23   \n",
       "7         1033          0.91              27   \n",
       "8         1033          0.85             253   \n",
       "9          974          0.87             226   \n",
       "10         881          0.86             228   \n",
       "11         860          0.92             286   \n",
       "12         862          0.97             159   \n",
       "13         859          0.90             246   \n",
       "14         851          0.96              73   \n",
       "15         767          0.98             116   \n",
       "16         731          0.98             234   \n",
       "17         710          0.87             597   \n",
       "18         688          0.98              38   \n",
       "19         663          0.98              72   \n",
       "\n",
       "                                             Post URL  Original Content  \\\n",
       "0                 https://i.redd.it/fk95v2ghilea1.png             False   \n",
       "1                 https://i.redd.it/1t5iwk9ymbwa1.jpg             False   \n",
       "2                 https://i.redd.it/x9hvdw9rdada1.jpg             False   \n",
       "3                 https://i.redd.it/bdmbfzxtvpka1.jpg             False   \n",
       "4   https://www.reddit.com/r/datascience/comments/...             False   \n",
       "5   https://www.reddit.com/r/datascience/comments/...             False   \n",
       "6                 https://i.redd.it/tppr6p77tqka1.png             False   \n",
       "7                 https://i.redd.it/2te69la63gca1.jpg             False   \n",
       "8                 https://i.redd.it/aspuqjwxhkpa1.png             False   \n",
       "9   https://www.reddit.com/r/datascience/comments/...             False   \n",
       "10  https://www.reddit.com/r/datascience/comments/...             False   \n",
       "11  https://www.reddit.com/r/datascience/comments/...             False   \n",
       "12  https://www.reddit.com/r/datascience/comments/...             False   \n",
       "13  https://www.reddit.com/r/datascience/comments/...             False   \n",
       "14                https://i.redd.it/ywof5cgsgnha1.jpg             False   \n",
       "15  https://www.reddit.com/r/datascience/comments/...             False   \n",
       "16  https://www.reddit.com/r/datascience/comments/...             False   \n",
       "17  https://www.reddit.com/r/datascience/comments/...             False   \n",
       "18                https://i.redd.it/i1oqiifhklva1.jpg             False   \n",
       "19  https://www.reddit.com/r/datascience/comments/...             False   \n",
       "\n",
       "                                        Top comment 1  \\\n",
       "0   He also has a PhD in mathematics so I'm sure t...   \n",
       "1   > _Yeah, well, that's just, like, your opinion...   \n",
       "2   Based on your chart it's 200k+ in the past six...   \n",
       "3   It's a shame you didn't copy the FAIRLY useful...   \n",
       "4   I've lost track of how many DS contracts are: ...   \n",
       "5   Very thorough analysis OP. I wonder if there‚Äôs...   \n",
       "6   If you open it in Excel first, he'd be Agent J...   \n",
       "7   I'm not a DS, I'm a fleshy Reinforcement Algor...   \n",
       "8   I love when people who have a relevant degree ...   \n",
       "9   I have a female cheer leader name. I once had ...   \n",
       "10  I'm surprised you don't ask if they can fix yo...   \n",
       "11  So I guess the obvious takeaway is that there'...   \n",
       "12  Some people here for some reason love to bitch...   \n",
       "13  If morality is something your struggling with,...   \n",
       "14  This ad was created by the previous NLP engineer.   \n",
       "15  Feel like this has been hyped forever. Excited...   \n",
       "16  Well my company just gave me completely unanon...   \n",
       "17  Hilarious ü§£ although i find scatter plots quit...   \n",
       "18  98% too high\\n\\n12% too low \\n\\n65% is clearly...   \n",
       "19                      Thanks for this helpful post!   \n",
       "\n",
       "                                        Top comment 2  \\\n",
       "0   I‚Äôm assuming he had a strong mathematical/stat...   \n",
       "1   \\nChart : Ambiguous\\nJob Title Ambiguities : e...   \n",
       "2   Someone else posted something similar but inst...   \n",
       "3   It's never matlab, why did my PhD supervisor m...   \n",
       "4              Data analytics.. that‚Äôs data analytics   \n",
       "5   Incredibly thorough analysis. And that's why I...   \n",
       "6   The further I get into ML and data engineering...   \n",
       "7                   What the hell man I just got here   \n",
       "8   I mean that would be great, if it was any less...   \n",
       "9   I had the same experience with legally changin...   \n",
       "10  Weirdly at the end of my physics degree we had...   \n",
       "11  How will I get my Data science job with all th...   \n",
       "12  It took me 8 years to get to six figures. What...   \n",
       "13  >Isn't that what your real mission is? Even in...   \n",
       "14  BAHAHAHAHA .. after all the recent layoffs the...   \n",
       "15                                          [deleted]   \n",
       "16  You would be absolutely horrified if you looke...   \n",
       "17  It's funny because I can answer most of these ...   \n",
       "18                                          [deleted]   \n",
       "19  The performance shittiness of pandas is a larg...   \n",
       "\n",
       "                                        Top comment 3  \n",
       "0   How seriously are personal projects taken? I'm...  \n",
       "1   I've done all of those things, in varying amou...  \n",
       "2      If my data viz was this bad I would deserve it  \n",
       "3   Php ü§£ü§£ü§£ has the author learned php and trying ...  \n",
       "4   True, but I would argue in general, if you're ...  \n",
       "5                 This is a great investigation. Wow.  \n",
       "6       That dummy 1 is obviously a float not an int.  \n",
       "7     I did not need to be called out like this today  \n",
       "8   It‚Äôs very r/restofthefuckingowl kind of advice...  \n",
       "9   What‚Äôs crazy is I have the opposite problem. I...  \n",
       "10  Is this a reference to some odd post in this s...  \n",
       "11  >Two were overqualified. Their experience was ...  \n",
       "12  Most of the people looking down on SQL Monkeys...  \n",
       "13  I felt exactly what you described in many inst...  \n",
       "14                                  US $?\\nOr meta $?  \n",
       "15  A million IT Security engineers suddenly and c...  \n",
       "16  That's more than mildly horrifying.  If you lo...  \n",
       "17  We use a P-value of 0.05 because R.A Fisher to...  \n",
       "18                                 ChatGPT gone wrong  \n",
       "19                                          Polars ‚úåÔ∏è  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code for step 2 goes here\n",
    "# create as many cells as necessary\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "subreddit = reddit.subreddit(\"datascience\")\n",
    "\n",
    "# Scraping the top posts of all time\n",
    "posts = subreddit.top(time_filter = \"all\",limit=None)\n",
    "\n",
    "#dictionary containing various post attributes\n",
    "posts_dict = {\"Post title\": [], \"Post Text\": [],\"Date\":[],\n",
    "              \"ID\": [], \"Post score\": [], \"Upvote Ratio\": [],\n",
    "              \"Total Comments\": [], \"Post URL\": [],\n",
    "              \"Original Content\": [], \"Top comment 1\":[], \n",
    "              \"Top comment 2\":[], \"Top comment 3\":[] }\n",
    "\n",
    "#setting start date as a timestamp\n",
    "start_date = '01-01-23 00:00:00'\n",
    "#start_date = datetime.datetime.strptime.timestamp()\n",
    "start_date = datetime.datetime.strptime(start_date, '%d-%m-%y %H:%M:%S').timestamp()\n",
    "\n",
    "\n",
    "for post in posts:\n",
    "    # Date of each posts' creation\n",
    "    date_creation = post.created_utc\n",
    "    #applying required conditions\n",
    "    if date_creation > start_date and post.score>50 and len((post.title).split(' '))>5 and post.num_comments>=3:\n",
    "        # Title of each post\n",
    "        posts_dict[\"Post title\"].append(post.title)\n",
    "     \n",
    "        # Text inside a post\n",
    "        posts_dict[\"Post Text\"].append(post.selftext)\n",
    "     \n",
    "        # Unique ID of each post\n",
    "        posts_dict[\"ID\"].append(post.id)\n",
    "     \n",
    "        # The score of a post\n",
    "        posts_dict[\"Post score\"].append(post.score)\n",
    "        \n",
    "        posts_dict[\"Upvote Ratio\"].append(post.upvote_ratio)\n",
    "     \n",
    "        # Total number of comments inside the post\n",
    "        posts_dict[\"Total Comments\"].append(post.num_comments)\n",
    "         \n",
    "        # Date the post was Created\n",
    "        date_and_time=pd.to_datetime(post.created_utc, unit='s')\n",
    "        dt_string=str(date_and_time)\n",
    "        date=dt_string[:10]\n",
    "        posts_dict[\"Date\"].append(date)\n",
    "        \n",
    "        # URL of each post\n",
    "        posts_dict[\"Post URL\"].append(post.url)\n",
    "        \n",
    "        # Flair of each post\n",
    "        posts_dict[\"Original Content\"].append(post.is_original_content)\n",
    "        \n",
    "        #Getting top 3 comments\n",
    "        comment_list=post.comments.list()\n",
    "        \n",
    "        comment1=comment_list[0]\n",
    "        comment2=comment_list[1]\n",
    "        comment3=comment_list[2]\n",
    "        \n",
    "        posts_dict[\"Top comment 1\"].append(comment1.body)\n",
    "        posts_dict[\"Top comment 2\"].append(comment2.body)\n",
    "        posts_dict[\"Top comment 3\"].append(comment3.body)\n",
    "    \n",
    "\n",
    "# Saving the data in a pandas dataframe\n",
    "all_posts = pd.DataFrame(posts_dict)\n",
    "all_posts.head(20)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7b2ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Checking number of posts collected\n",
    "all_posts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136d051",
   "metadata": {},
   "source": [
    "## Step 3: \n",
    "### 10 points\n",
    "\n",
    "Save the data on your local disk. You may have used lists or similar data structures for the intial porcessing. Convert that data structure into a Pandas dataframe. Save the dataframe as a .csv file into your local disk. \n",
    "\n",
    "Here are the column details:\n",
    "\n",
    "Column 1: Date\n",
    "\n",
    "Column 2: Post score\n",
    "\n",
    "Column 3: Post title\n",
    "\n",
    "Column 4: Top comment 1\n",
    "\n",
    "Column 5: Top comment 2\n",
    "\n",
    "Column 6: Top comment 3\n",
    "\n",
    "When you create the .csv file, it should have 101 rows (including column names) and 6 columns.\n",
    "\n",
    "**Grading criteria:**\n",
    "If your code produces a .csv file in the local disk in the same folder as the Jupyter Notebook file, you get full points. Otherwise, no point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee47057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for step 3 goes here\n",
    "# create as many cells as necessary\n",
    "\n",
    "#creating appropriate dataframe having required columns \n",
    "final_df=all_posts.filter(items=['Date','Post score','Post title','Top comment 1', 'Top comment 2', 'Top comment 3']).head(100)\n",
    "final_df\n",
    "\n",
    "#converting dataframe to a csv file\n",
    "final_df.to_csv('isha_jain_casestudy1.csv', encoding='utf-8',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b12bd",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "### 10 points\n",
    "#### Presentation slides:\n",
    "   \n",
    "Create presentation slides for this case study. The presentation slides should provide an overview of the problem you tried to solve, methods you have used (don't put actual code in the slides), and if you have discovered new insights from the data you have collected. You may put actual post titles or comments in the slide that you found insightful and interesting. The number of slides should be around 6-7 (no hard limit). Three of you will be randomly chosen and be asked to present your work in the class. You should be prepared to present your work for 5 mins.\n",
    "\n",
    "\n",
    "**Notes on grading**: 5 points will be deducted if you are not prepared to present on the day of submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e062707",
   "metadata": {},
   "source": [
    "### What to submit:\n",
    "\n",
    "All files should be named in the following format:\n",
    "\n",
    "firstname_lastname_casestudy_1.pdf\n",
    "\n",
    "firstname_lastname_casestudy_1.ipynb\n",
    "\n",
    "firstname_lastname_casestudy_1.csv\n",
    "\n",
    "etc.\n",
    "\n",
    "\n",
    "Put the Jupyter Notebook file and the .csv file in a folder. Then convert your presentation slides to a PDF file and put it in the same folder. Zip the folder. After zipping, it should have the extension .zip. The name of the .zip file should be firstname_lastname_casestudy_1.zip . Upload the .zip file on Canvas.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
